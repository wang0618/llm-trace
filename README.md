# LLM Path

A lightweight tool for tracing LLM API requests — intercept, record, and visualize your LLM application's behavior.

## Features

- **Transparent Proxy** — Drop-in HTTP proxy that captures all LLM API traffic. Works with any OpenAI-compatible API and Anthropic API.
- **Request Visualization** — Interactive web viewer to visualize the requests topology graph and show the context diff between requests.

## Installation

```bash
pip install llm-path
```

## Quick Start

### 1. Start the Proxy

```bash
llm-path proxy --port 8080 --target https://api.openai.com --output trace.jsonl
```

Replace the `--target` host in the command above with your LLM provider's API host.

### 2. Point Your Client to the Proxy

```diff
from openai import OpenAI

- client = OpenAI()
+ client = OpenAI(base_url="http://localhost:8080/v1")
```

All requests will be transparently forwarded to your LLM provider and recorded to the trace file.

### 3. Visualize the Traces

```bash
llm-path viewer trace.jsonl
```

## CLI Reference

```bash
# Start proxy server
llm-path proxy [OPTIONS]
  --port      Port to listen on (default: 8080)
  --output    Output JSONL file path (required)
  --target    LLM Provider API URL (required)

# Visualize traces
llm-path viewer <input> [OPTIONS]
  --port      Port to listen on (default: 8765)
  --host      Host to bind to (default: 127.0.0.1)
```

## License

MIT
